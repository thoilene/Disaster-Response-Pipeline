{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster Management App - Using ML Pipeline \n",
    "\n",
    "For the purpose of disaster management, disaster messages are collected and categorized . An important aspect of disaster management is the choice of the disaster categories which allow to take appropriate measures to manage the situstion. In case of accident, fire or other immediate threats, a reaction is expected within a few minutes. \n",
    "\n",
    "**How could AI and Machine Learning support disaster management in this context?**\n",
    "\n",
    "A disaster response App using Machine Learning to classify disaster messages could be the answer to the question above. In this solution, a multi-class Machine Learning classifier could classify the disaster messages using NLP techniques. The outcoume of the classification would be made available to the disaster management office, which will take the appropriate measures to manage the situation.The solution proposed in this work contains three parts.\n",
    "\n",
    "- **ETL Pipeline for data preparation (ETL-Pipeline-Preparation.ipynb)**\n",
    "\n",
    "The ETL pipeline will prepare the data to make it clean for machine learning. The data are read from csv-files, tranformed and stored in a database-file.\n",
    "\n",
    "- **Multi-Classes Machine Learning Pipeline (Disaster-Response-ML-Pipeline.ipynb)**\n",
    "\n",
    "The multi-classes ML pipeline will read the disaster messages and thier categories from database-file mentionned above. An ML model will be built, trainned and stored on the local filesystem.\n",
    "\n",
    "- **Flask web-App for the categorization of disaster messages (./app/run.py)**\n",
    "\n",
    "The Flask web-app will load the ML model from the filesystem and the disaster messages from the database created with the ETL pipeline. It will offer a functionality to select a message and visualize the classes of the message instantly. This information could then be used by disaster management office. \n",
    "\n",
    "\n",
    "The [disaster messages](https://www.kaggle.com/davidshahshankhar/disasterresponsepipeline) dataset supporting this work is freely available on kaggle.com. It consists of 2 csv-files:\n",
    "\n",
    "- messages.csv: file containing disater messages\n",
    "- categories.csv: file containing different categories of disaster \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Herkules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Herkules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Herkules\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet','stopwords'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline for data preparation (./ETL-Pipeline-Preparation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    MESSAGES: messages.csv\n",
      "    CATEGORIES: categories.csv\n",
      "Cleaning data...\n",
      "Saving data...\n",
      "    DATABASE: DisasterResponse.db\n",
      "Cleaned data saved to database!\n"
     ]
    }
   ],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \n",
    "    # load messages dataset\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "    \n",
    "    # load categories dataset\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "    \n",
    "    # merge datasets\n",
    "    df = messages.merge(categories, on=[\"id\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \n",
    "    # create a dataframe of the 36 individual category columns\n",
    "    categories = df.categories.str.split(\";\",expand=True)\n",
    "\n",
    "    # select the first row of the categories dataframe\n",
    "    row = categories.head(1)\n",
    "\n",
    "    # use this row to extract a list of new column names for categories.\n",
    "    # one way is to apply a lambda function that takes everything \n",
    "    # up to the second to last character of each string with slicing\n",
    "    category_colnames = list(map(lambda x: x[:-2] , list(row.values[0])) )\n",
    "\n",
    "    # rename the columns of `categories`\n",
    "    categories.columns = category_colnames\n",
    "\n",
    "    for column in categories.columns:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].astype(str).str[-1]\n",
    "\n",
    "        # convert column from string to numeric\n",
    "        categories[column] = categories[column].astype(int)\n",
    "\n",
    "    # drop the original categories column from `df`\n",
    "    df.drop('categories',axis=1,inplace=True)\n",
    "\n",
    "    # concatenate the original dataframe with the new `categories` dataframe\n",
    "    df = pd.concat([df, categories], axis=1)\n",
    "\n",
    "    # drop duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, database_filename):\n",
    "    \n",
    "    engine = create_engine('sqlite:///{:}.db'.format(database_filename))\n",
    "    df.to_sql('DisasterResponseMessages', engine, if_exists='replace', index=False)  \n",
    "\n",
    "\n",
    "def run_etl_pipeline(messages_filepath, categories_filepath, database_filepath):\n",
    "    try:\n",
    "\n",
    "        print('Loading data...\\n    MESSAGES: {}\\n    CATEGORIES: {}'\n",
    "              .format(messages_filepath, categories_filepath))\n",
    "        df = load_data(messages_filepath, categories_filepath)\n",
    "\n",
    "        print('Cleaning data...')\n",
    "        df = clean_data(df)\n",
    "        \n",
    "        print('Saving data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        save_data(df, database_filepath)\n",
    "        \n",
    "        print('Cleaned data saved to database!')\n",
    "    \n",
    "    except:\n",
    "        print('Please provide the filepaths of the messages and categories '\\\n",
    "              'datasets as the first and second argument respectively, as '\\\n",
    "              'well as the filepath of the database to save the cleaned data '\\\n",
    "              'to as the third argument. \\n\\nExample: disaster_messages.csv'\\\n",
    "              'disaster_categories.csv DisasterResponse.db')\n",
    "run_etl_pipeline(messages_filepath='messages.csv', categories_filepath='categories.csv', database_filepath='DisasterResponse.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Classes Machine Learning Pipeline (./Disaster-Response-ML-Pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: data/DisasterResponse.db\n",
      "Building model...\n",
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "def load_data(database_filepath):\n",
    "    \"\"\"\n",
    "    Funtion to load data from database\n",
    "    Parameter: \n",
    "        database_filepath: path of the database \n",
    "    Returns:\n",
    "        - X: features (messages)\n",
    "        - Y: Labels (categories)\n",
    "        - categories_names: columns of the labels\n",
    "    \"\"\" \n",
    "    engine = create_engine('sqlite:///{:}'.format(database_filepath))\n",
    "    df = pd.read_sql_table(\"DisasterResponseMessages\", con=engine)\n",
    "    X = df['message']\n",
    "    Y = df.drop(['id','message','original','genre'],axis=1)\n",
    "    \n",
    "    return X, Y, Y.columns\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" \n",
    "    Function to transform text in tokens\n",
    "    parameters:\n",
    "        text: text to be transformed\n",
    "    Return:\n",
    "        list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "    tokens = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Function to build a model for machine learning\n",
    "    return:\n",
    "        model for machine learning\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', TfidfVectorizer(tokenizer=tokenize)),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators= 50, random_state=42)))\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"\n",
    "    Function to evaluate a model\n",
    "    Parameters:\n",
    "        model: The model to be evaluated\n",
    "        X_test: features for testing\n",
    "        Y_test: labels for testing\n",
    "        category_names: columns of the labels\n",
    "    \"\"\"\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    for i, col in enumerate(category_names):\n",
    "\n",
    "            accuracy=accuracy_score(Y_test.loc[:,col],Y_pred[:,i])\n",
    "            print(\"[{:}] - accuracy: {:.2f}\\n\".format(col,accuracy))\n",
    "            print(classification_report(Y_test[col], Y_pred[:, i]))\n",
    "\n",
    "\n",
    "def save_model(model, model_filepath):\n",
    "    \"\"\"\n",
    "    Function to save a model\n",
    "    Parameters:\n",
    "        model: model to be saved\n",
    "        model_filepath: file path where the model will be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    with open('model_filepath', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def run_ML_pipeline(database_filepath, model_filepath):\n",
    "    try:\n",
    "        print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        X, Y, category_names = load_data(database_filepath)\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        print('Building model...')\n",
    "        model = build_model()\n",
    "        \n",
    "        print('Training model...')\n",
    "        model.fit(X_train, Y_train)\n",
    "        \n",
    "        print('Evaluating model...')\n",
    "        evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "        print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "        save_model(model, model_filepath)\n",
    "\n",
    "        print('Trained model saved!')\n",
    "\n",
    "    except:\n",
    "        print('Please provide the filepath of the disaster messages database '\\\n",
    "              'as the first argument and the filepath of the pickle file to '\\\n",
    "              'save the model to as the second argument. \\n\\nExample: '\\\n",
    "              './data/DisasterResponse.db classifier.pkl')\n",
    "\n",
    "run_ML_pipeline(database_filepath='data/DisasterResponse.db', model_filepath=\"models/classifier.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask web-App for the categorization of disaster messages (./app/run.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "\n",
    "import json\n",
    "import plotly\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from flask import Flask\n",
    "from flask import render_template, request, jsonify\n",
    "from plotly.graph_objs import Bar\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "# load data\n",
    "engine = create_engine('sqlite:///./data/DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponseMessages', engine)\n",
    "\n",
    "# load model\n",
    "model = joblib.load(\"./models/classifier.pkl\")\n",
    "\n",
    "\n",
    "# index webpage displays cool visuals and receives user input text for model\n",
    "@app.route('/')\n",
    "@app.route('/index')\n",
    "def index():\n",
    "    \n",
    "    # extract data needed for visuals\n",
    "    # TODO: Below is an example - modify to extract data for your own visuals\n",
    "    genre_counts = df.groupby('genre').count()['message']\n",
    "    genre_names = list(genre_counts.index)\n",
    "    \n",
    "    # create visuals\n",
    "    # TODO: Below is an example - modify to create your own visuals\n",
    "    graphs = [\n",
    "        {\n",
    "            'data': [\n",
    "                Bar(\n",
    "                    x=genre_names,\n",
    "                    y=genre_counts\n",
    "                )\n",
    "            ],\n",
    "\n",
    "            'layout': {\n",
    "                'title': 'Distribution of Message Genres',\n",
    "                'yaxis': {\n",
    "                    'title': \"Count\"\n",
    "                },\n",
    "                'xaxis': {\n",
    "                    'title': \"Genre\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # encode plotly graphs in JSON\n",
    "    ids = [\"graph-{}\".format(i) for i, _ in enumerate(graphs)]\n",
    "    graphJSON = json.dumps(graphs, cls=plotly.utils.PlotlyJSONEncoder)\n",
    "    \n",
    "    # render web page with plotly graphs\n",
    "    return render_template('master.html', ids=ids, graphJSON=graphJSON)\n",
    "\n",
    "\n",
    "# web page that handles user query and displays model results\n",
    "@app.route('/go')\n",
    "def go():\n",
    "    # save user input in query\n",
    "    query = request.args.get('query', '') \n",
    "\n",
    "    # use model to predict classification for query\n",
    "    classification_labels = model.predict([query])[0]\n",
    "    classification_results = dict(zip(df.columns[4:], classification_labels))\n",
    "\n",
    "    # This will render the go.html Please see that file. \n",
    "    return render_template(\n",
    "        'go.html',\n",
    "        query=query,\n",
    "        classification_result=classification_results\n",
    "    )\n",
    "\n",
    "\n",
    "def run_app():\n",
    "    app.run(host='127.0.0.1', port=3001, debug=True)\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run disaster web-app**\n",
    "- go to terminal at working directory\n",
    "- change directory: cd app\n",
    "- run app: python run.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently enter the url http://127.0.0.1:3001/ to your browser and start the app on localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Disaster Home](pic1.jpg \"Disaster Home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Disaster Messages](pic2.jpg \"Disaster Message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Disaster Messages](pic3.jpg \"Disaster Message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
